# This file was created to scrape the Baseball Reference webpage to collect all
# necessary information for the team database. It makes use of scrapy to scrape
# the websites, then attempts to automatically assign the data to the team's name
# and location. Can manually modify this if needed.

import time
import scrapy
from scrapy.crawler import CrawlerProcess
import sqlite3

class BaseballSpider(scrapy.Spider):

    name = "baseball-spider"

    def start_requests(self):
        url = "https://www.baseball-reference.com/leagues/majors/2025.shtml"
        yield scrapy.Request(url=url, callback=self.parse_league)
    
    def parse_league(self, response):
        table = response.css("table#teams_standard_batting")
        elems = table.xpath("./tbody[1]//th[@data-stat='team_name']/a")
        all_names.extend(elems.xpath("./text()").extract())
        links = elems.xpath("./@href").extract()
        for link in links:
            time.sleep(1)
            team_code = link.split("/")[2]
            all_codes.append(team_code)
            yield response.follow(url = link, callback = self.parse_teams, cb_kwargs = dict(code=team_code))

    def parse_teams(self, response, code):
        # get batters 
        time.sleep(1)
        table = response.css("table#players_standard_batting")
        trs = table.xpath("./tbody/tr[position() < 10]")
        tds = trs.xpath("./td[@data-stat='name_display']")
        ids = tds.xpath("./@data-append-csv").extract()
        names = tds.xpath("./a/text()").extract()
        positions = trs.xpath("./td[@data-stat='team_position']/strong/text()").extract()
        
        # get pitchers
        table = response.css("table#players_standard_pitching")
        trs = table.xpath("./tbody/tr")
        for tr in trs:
            if len(tr.xpath(".//strong")) > 0:
                td = tr.xpath("./td[@data-stat='name_display']")
                id = td.xpath("./@data-append-csv").extract_first()
                name = td.xpath("./a/text()").extract_first()
                pos = tr.xpath("./td[@data-stat='team_position']/strong/text()").extract_first()
                ids.append(id)
                names.append(name)
                positions.append(pos)
        
        # add information
        player_dict[code] = {"ids" : ids, "names": names, "positions":positions}

# collect all team codes and team names
all_codes = []
all_names = []
player_dict = dict()

process = CrawlerProcess()
process.crawl(BaseballSpider)
process.start()

# begin inserting these teams into database
conn = sqlite3.connect("C:/Users/nmbr1/OneDrive/Documents/bases-loaded-baseball/player_stats.db")
cursor = conn.cursor()

# delete table to reset them
cursor.execute("DROP TABLE teams;")

# create table for teams
comm = """CREATE TABLE teams (
team_code VARCHAR(3) PRIMARY KEY,
location VARCHAR(15),
name VARCHAR(18) NOT NULL);"""

cursor.execute(comm)

# add teams into database
while len(all_codes) > 0:
    # attempt to automatically identify information
    code = all_codes.pop(0).strip()
    team = all_names.pop(0)
    try:
        [loc, name] = team.split(maxsplit=1)
        loc, name = loc.strip(), name.strip()
    except ValueError:  # for Athletics
        name = team.strip()
        loc = ""

    # print info
    print("\nAbout to add the following entry:")
    print(f"Code: {code}")
    print(f"Team: {loc}")
    print(f"Name: {name}")
        
    # ask if correct
    confirmed = False
    while not confirmed:
        print("\nIs this correct?")
        sel = input ("Type 'y' for yes, or 'n' for no >> ")
        if sel == "y":
            confirmed = True
        elif sel == "n":    # correct the information if needed
            [loc, name] = team.rsplit(maxsplit=1)
            loc, name = loc.strip(), name.strip()
            confirmed = True
        else:
            print("Please type 'y' or 'n'.")
    
    # add the team
    cursor.execute(f"INSERT INTO teams VALUES ('{code}', '{loc}', '{name}');")
    print(f"{name} added.")
    
conn.commit()
conn.close()

# test the dictionary
for code in player_dict:
    players = player_dict[code]
    while len(players["ids"]) > 0:
        id = players["ids"].pop(0)
        name = players["names"].pop(0)
        pos = players["positions"].pop(0)
        first, last = name.split(maxsplit=1)
        print(f"{code} | {id:9} | {pos:2} | {first.strip()} {last.strip()}")